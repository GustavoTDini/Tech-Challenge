{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavoTDini/Tech-Challenge/blob/main/FIAP_Amazon_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning do Banco de Dados da Amazon\n",
        "\n",
        "Faremos um fine tuning para retornar a descrição de um produto com base no banco de dados da amazon"
      ],
      "metadata": {
        "id": "8P-jrqiFu-LA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialmente vamos Conectar o notebook com o Google Drive, pois como o arquivo e muito grande, temos que te-lo salvo em algum lugar de fácilo acesso."
      ],
      "metadata": {
        "id": "VTbSe94lvR3i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrDgUwgFFDJt"
      },
      "outputs": [],
      "source": [
        "#Conexão com o Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos instalar as dependencias necessárias"
      ],
      "metadata": {
        "id": "YEuwtwbAvhDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NBMXaEKEqmfN"
      },
      "outputs": [],
      "source": [
        "# Vamos atualizar inicialmente o pip para podermos instalar as dependecias com segurança\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oX_wY5pzq45C"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-deps 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c8nSaGIMtsRc"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps torch xformers trl peft accelerate bitsandbytes triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth_zoo"
      ],
      "metadata": {
        "id": "8EYhfgt6p3qk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Preparação do dataset\n",
        "\n",
        "Agora vamos preparar nossa base de dados, vamos pegar o arquivo do database e trata-lo e transformar em um json pronto para ser lido pelo Python"
      ],
      "metadata": {
        "id": "KYwb10HTwGJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHgihYEVyta6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/FIAP/IA/fine_tuning/trn.json\"\n",
        "\n",
        "def get_inicial_data(file_path):\n",
        "    # Iremos iniciar abrindo duas novas coleção no python, uma para os titulos e outra para as descrições\n",
        "    title_data = []\n",
        "    content_data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Agora para cada linha testamos o Json se é valido\n",
        "        for line in file:\n",
        "            try:\n",
        "                json_object = json.loads(line)\n",
        "                title = json_object[\"title\"]\n",
        "                content = json_object[\"content\"]\n",
        "                # iremos eliminar os casos duplicados\n",
        "                if (title != '' and content != ''):\n",
        "                    title_data.append(title)\n",
        "                    content_data.append(content)\n",
        "            except json.JSONDecodeError:\n",
        "                # e Caso o Json seja invalido ele não será incluido\n",
        "                print(f\"Eliminando linha com Json invalido: {line.strip()}\")\n",
        "                continue\n",
        "\n",
        "    return title_data, content_data\n",
        "\n",
        "# salvamos essas 2 listas em variaveis python\n",
        "title_list, content_list = get_inicial_data(json_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5tfg8t0okCb"
      },
      "outputs": [],
      "source": [
        "# Checaremos quantas linhas há em cada lista e se são equivalentes\n",
        "print(len(title_list))\n",
        "print(len(content_list))\n",
        "\n",
        "# Vamos checar mostrando as 10 primeiras lindas de cada Json\n",
        "for i in range(1,10):\n",
        "    print(title_list[i])\n",
        "    print(content_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxEcHGTDYewU"
      },
      "outputs": [],
      "source": [
        "# Agora vamos salvar os dados em um outro Json, para evitar ter que tratar os dados novamente caso precisemos reiniciar o treinamento\n",
        "json_titles_path = \"/content/drive/MyDrive/FIAP/IA/fine_tuning/titles.json\"\n",
        "json_contents_path = \"/content/drive/MyDrive/FIAP/IA/fine_tuning/contents.json\"\n",
        "\n",
        "with open(json_titles_path, 'w') as json_file:\n",
        "    json.dump(title_list, json_file)\n",
        "\n",
        "with open(json_contents_path, 'w') as json_file:\n",
        "    json.dump(content_list, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E com a biblioteca pandas, vamos transformar essas 2 listas em um dataframe"
      ],
      "metadata": {
        "id": "F_-MMcF4xdPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NipklNjKdGEn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "with open(json_titles_path, 'r') as json_file:\n",
        "    data_titles = json.load(json_file)\n",
        "\n",
        "with open(json_contents_path, 'r') as json_file:\n",
        "    data_contents = json.load(json_file)\n",
        "\n",
        "# Agora com o pandas, vamos transformar as duas listas de Json em um dataframe\n",
        "data = {'title': data_titles, 'content': data_contents}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para finalmente chegarmos a um dataset que pode ser utilizado para o fine-tuning"
      ],
      "metadata": {
        "id": "KgqbpKl0xz63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW5maX8Bh3ND"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = pd.DataFrame(df)\n",
        "dataset = Dataset.from_pandas(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "TFSDn2UO1CXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2 - Carregar o foundation model\n",
        "\n",
        "Agora iremos carregar o modelo que será a base do nosso fine-tuning atraves do unsloth"
      ],
      "metadata": {
        "id": "rRO6KI-KyPDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJnCPN3oZucw"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Carregaremos o modelo inicial atraves do unsloth, no nosso caso iremos usar o modelo Llama de 8 bilhões de parametros\n",
        "checkpoint_modelo = 'unsloth/Meta-Llama-3.1-8B'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVP5xs7fQeI"
      },
      "outputs": [],
      "source": [
        "# Carregando o modelo e o tokenizador pré-treinados com parametros equilibrados entre tempo de computação e eficiencia\n",
        "# Colocamos a sequencia maxima de tokens de 4096 para uma descrição robusta\n",
        "# Dtype sendo none a detecção é automatica\n",
        "# e vamos carregar em 4bits para otimizar a memória\n",
        "modelo, tokenizador = FastLanguageModel.from_pretrained(\n",
        "    model_name = checkpoint_modelo,\n",
        "    max_seq_length = 4096,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo"
      ],
      "metadata": {
        "id": "B_ycbBKPsbBd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizador"
      ],
      "metadata": {
        "id": "lo2-SDK9scen",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 3 - Testando o modelo Inicial\n",
        "\n",
        "Agora com o modelo carregado, iremos testá-lo com um prompt"
      ],
      "metadata": {
        "id": "zxHvygC0y_3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDPkR1W1px-Z"
      },
      "outputs": [],
      "source": [
        "prompt = \"Please, describe Lord of the Rings\"\n",
        "\n",
        "prompt_tokenizado = tokenizador([prompt], return_tensors = \"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokenizado"
      ],
      "metadata": {
        "id": "MMvNRkl3t-N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer"
      ],
      "metadata": {
        "id": "o7PNpDTQuHaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(modelo)\n",
        "\n",
        "for i in range(10):\n",
        "    streamer_texto = TextStreamer(tokenizador)\n",
        "    _ = modelo.generate(**prompt_tokenizado, streamer = streamer_texto, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "3H3CYD_vuLwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 4 - Gerando o prompt de teste\n",
        "\n",
        "Agora iremos criar um template de prompt para termos melhores resultados - vamos utilizar a padronização do unsloth"
      ],
      "metadata": {
        "id": "7-RmEPD90Yj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx4vNSXngPtR"
      },
      "outputs": [],
      "source": [
        "prompt_instruction = 'You are an AI assistant that helps people find information. Based on the title, or name of the product, you will generate a description of the product.'\n",
        "\n",
        "def prompt_input(input):\n",
        "    return f\"Please, describe {input}\"\n",
        "\n",
        "\n",
        "def gerar_prompt(title, content=\"\"):\n",
        "      return f\"{prompt_instruction}\\n\\n### Input:\\n{prompt_input(title)}\\n\\n### Response:\\n{content}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIxw5IAwhW3t"
      },
      "outputs": [],
      "source": [
        "# Testando o prompt com a primeira linha do nosso dataset\n",
        "print(gerar_prompt(dataset[0]['title'], dataset[0]['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0auyGBKjRv8"
      },
      "outputs": [],
      "source": [
        "#Precisaremos de um string marcador de fim de sentença\n",
        "EOS_TOKEN = tokenizador.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVTh3RXOjVaC"
      },
      "outputs": [],
      "source": [
        "#Agora vamos usar nosso dataset para gerar os prompts para isso criaremos ums função que crie essa\n",
        "def gerar_todos_os_prompts(data):\n",
        "    titles = data['title']\n",
        "    contents = data['content']\n",
        "    textos = []\n",
        "    for title, content in zip(titles, contents):\n",
        "        texto = gerar_prompt(title, content) + EOS_TOKEN\n",
        "        textos.append(texto)\n",
        "    return  textos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 5 - Realizando o Fine-Tuning\n",
        "\n",
        "E Finalmente com todos os ajustes realizados vamos realizar o fine-tuning"
      ],
      "metadata": {
        "id": "IUsAmkEF3FCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHo_8tIsmTNR"
      },
      "outputs": [],
      "source": [
        "# utilizando a biblioteca FastLanguageModel - iremos utilizar a funçao\n",
        "# get_peft_model para definir os parametros que serão treinados no fine-tuning\n",
        "# economizando assim, tempo computacional\n",
        "# PEFT significa Parameter-Efficient Fine-Tuning\n",
        "modelo_treinado = FastLanguageModel.get_peft_model(\n",
        "    modelo,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 10,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07bIxQEmmdKe"
      },
      "outputs": [],
      "source": [
        "# importaremos as bibliotecas para a realização do treinamento\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gJpNKW0mhve"
      },
      "outputs": [],
      "source": [
        "# e vamos aplicar tudo no SFTTrainer para realizar o Fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model = modelo_treinado,\n",
        "    tokenizer = tokenizador,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"texto\",\n",
        "    max_seq_length = 4096,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    peft_config = modelo.peft_config,\n",
        "    formatting_func = gerar_todos_os_prompts,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 500,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 10,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"none\", # Add this line to disable wandb logging\n",
        "    ),\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o modelo treinado - vamos fazer um teste com o mesmo prompt anterior"
      ],
      "metadata": {
        "id": "wZcl25xp5B_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(modelo)\n",
        "prompt_tokenizado = tokenizador(\n",
        "[gerar_prompt(\"Lord of the Rings\")], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "for i in range(10):\n",
        "    streamer_texto = TextStreamer(tokenizador)\n",
        "    _ = modelo_treinado.generate(**prompt_tokenizado, streamer = streamer_texto, max_new_tokens = 256)"
      ],
      "metadata": {
        "id": "_yYpqc3y36rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 6 - Salvando o modelo para uso\n",
        "\n",
        "Agora com o modelo treinado e testado - vamos salvar como uma biblioteca unsloth, para podermos acessar por outros metodos, como por exemplo o LMStudio"
      ],
      "metadata": {
        "id": "quI5JIIY5iaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "UT8lm5aPu3B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora vamos salvar o modelo localmente\n",
        "modelo_treinado.save_pretrained('modelo')\n",
        "tokenizador.save_pretrained('modelo')"
      ],
      "metadata": {
        "id": "Zb4YXx7YLyou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E podemos com esse mesmo modelo - publicar no hugging face\n",
        "modelo_treinado.push_to_hub('llama-3.1-8B-amazon-content')\n",
        "tokenizador.push_to_hub('llama-3.1-8B-amazon-content')"
      ],
      "metadata": {
        "id": "KNCxaYjGvAHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_treinado.save_pretrained_merged(\"modelo_merged\", tokenizador, save_method = \"merged_16bit\",)"
      ],
      "metadata": {
        "id": "VyVz7BT9R33z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1qWL8-76G5uQ4Ja1-OO_EDx2oz4vJF6Oj",
      "authorship_tag": "ABX9TyPEw8l5SP7btIjpzbMJRpPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}